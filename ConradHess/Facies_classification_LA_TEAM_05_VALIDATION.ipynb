{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facies classification using Machine Learning #\n",
    "## LA Team Submission 5 ## \n",
    "### _[Lukas Mosser](https://at.linkedin.com/in/lukas-mosser-9948b32b/en), [Alfredo De la Fuente](https://pe.linkedin.com/in/alfredodelafuenteb)_ ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach for solving the facies classfication problem ( https://github.com/seg/2016-ml-contest. ) we will explore the following statregies:\n",
    "- Features Exploration: based on [Paolo Bestagini's work](https://github.com/seg/2016-ml-contest/blob/master/ispl/facies_classification_try02.ipynb), we will consider imputation, normalization and augmentation routines for the initial features.\n",
    "- Model tuning: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "We will need to install the following libraries and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%sh\n",
    "# pip install pandas\n",
    "# pip install scikit-learn\n",
    "# pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold , StratifiedKFold\n",
    "# from classification_utilities import display_cm, display_adj_cm\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.signal import medfilt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "data = pd.read_csv('../facies_vectors.csv')\n",
    "# data = pd.read_csv('../ShiangYong/facies_vectors_imputedPE.csv')\n",
    "# Parameters\n",
    "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "facies_names = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS', 'WS', 'D', 'PS', 'BS']\n",
    "facies_colors = ['#F4D03F', '#F5B041','#DC7633','#6E2C00', '#1B4F72','#2E86C1', '#AED6F1', '#A569BD', '#196F3D']\n",
    "\n",
    "# data.dropna(inplace=True)\n",
    "# Store features and labels\n",
    "X = data[feature_names].values \n",
    "y = data['Facies'].values \n",
    "\n",
    "# Store well labels and depths\n",
    "well = data['Well Name'].values\n",
    "depth = data['Depth'].values\n",
    "# X = np.array(pd.DataFrame(X).dropna())\n",
    "# Fill 'PE' missing values with mean\n",
    "imp = preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4149 entries, 0 to 4148\n",
      "Data columns (total 11 columns):\n",
      "Facies       4149 non-null int64\n",
      "Formation    4149 non-null object\n",
      "Well Name    4149 non-null object\n",
      "Depth        4149 non-null float64\n",
      "GR           4149 non-null float64\n",
      "ILD_log10    4149 non-null float64\n",
      "DeltaPHI     4149 non-null float64\n",
      "PHIND        4149 non-null float64\n",
      "PE           3232 non-null float64\n",
      "NM_M         4149 non-null int64\n",
      "RELPOS       4149 non-null float64\n",
      "dtypes: float64(7), int64(2), object(2)\n",
      "memory usage: 356.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We procceed to run [Paolo Bestagini's routine](https://github.com/seg/2016-ml-contest/blob/master/ispl/facies_classification_try02.ipynb) to include a small window of values to acount for the spatial component in the log analysis, as well as the gradient information with respect to depth. This will be our prepared training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature windows concatenation function\n",
    "def augment_features_window(X, N_neig):\n",
    "    \n",
    "    # Parameters\n",
    "    N_row = X.shape[0]\n",
    "    N_feat = X.shape[1]\n",
    "\n",
    "    # Zero padding\n",
    "    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))\n",
    "\n",
    "    # Loop over windows\n",
    "    X_aug = np.zeros((N_row, N_feat*(2*N_neig+1)))\n",
    "    for r in np.arange(N_row)+N_neig:\n",
    "        this_row = []\n",
    "        for c in np.arange(-N_neig,N_neig+1):\n",
    "            this_row = np.hstack((this_row, X[r+c]))\n",
    "        X_aug[r-N_neig] = this_row\n",
    "\n",
    "    return X_aug\n",
    "\n",
    "\n",
    "# Feature gradient computation function\n",
    "def augment_features_gradient(X, depth):\n",
    "    \n",
    "    # Compute features gradient\n",
    "    d_diff = np.diff(depth).reshape((-1, 1))\n",
    "    d_diff[d_diff==0] = 0.001\n",
    "    X_diff = np.diff(X, axis=0)\n",
    "    X_grad = X_diff / d_diff\n",
    "        \n",
    "    # Compensate for last missing value\n",
    "    X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))\n",
    "    \n",
    "    return X_grad\n",
    "\n",
    "\n",
    "# Feature augmentation function\n",
    "def augment_features(X, well, depth, N_neig=1):\n",
    "    \n",
    "    # Augment features\n",
    "    X_aug = np.zeros((X.shape[0], X.shape[1]*(N_neig*2+2)))\n",
    "    for w in np.unique(well):\n",
    "        w_idx = np.where(well == w)[0]\n",
    "        X_aug_win = augment_features_window(X[w_idx, :], N_neig)\n",
    "        X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])\n",
    "        X_aug[w_idx, :] = np.concatenate((X_aug_win, X_aug_grad), axis=1)\n",
    "    \n",
    "    # Find padded rows\n",
    "    padded_rows = np.unique(np.where(X_aug[:, 0:7] == np.zeros((1, 7)))[0])\n",
    "    \n",
    "    return X_aug, padded_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_aug, padded_rows = augment_features(X, well, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Initialize model selection methods\n",
    "# lpgo = LeavePGroupsOut(2)\n",
    "\n",
    "# # Generate splits\n",
    "# split_list = []\n",
    "# for train, val in lpgo.split(X, y, groups=data['Well Name']):\n",
    "#     hist_tr = np.histogram(y[train], bins=np.arange(len(facies_names)+1)+.5)\n",
    "#     hist_val = np.histogram(y[val], bins=np.arange(len(facies_names)+1)+.5)\n",
    "#     if np.all(hist_tr[0] != 0) & np.all(hist_val[0] != 0):\n",
    "#         split_list.append({'train':train, 'val':val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \n",
    "    # Preprocess data to use in model\n",
    "    X_train_aux = []\n",
    "    X_test_aux = []\n",
    "    y_train_aux = []\n",
    "    y_test_aux = []\n",
    "    \n",
    "    # For each data split\n",
    "    split = split_list[5]\n",
    "        \n",
    "    # Remove padded rows\n",
    "    split_train_no_pad = np.setdiff1d(split['train'], padded_rows)\n",
    "\n",
    "    # Select training and validation data from current split\n",
    "    X_tr = X_aug[split_train_no_pad, :]\n",
    "    X_v = X_aug[split['val'], :]\n",
    "    y_tr = y[split_train_no_pad]\n",
    "    y_v = y[split['val']]\n",
    "\n",
    "    # Select well labels for validation data\n",
    "    well_v = well[split['val']]\n",
    "\n",
    "    # Feature normalization\n",
    "    scaler = preprocessing.RobustScaler(quantile_range=(25.0, 75.0)).fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_v = scaler.transform(X_v)\n",
    "        \n",
    "    X_train_aux.append( X_tr )\n",
    "    X_test_aux.append( X_v )\n",
    "    y_train_aux.append( y_tr )\n",
    "    y_test_aux.append (  y_v )\n",
    "    \n",
    "    X_train = np.concatenate( X_train_aux )\n",
    "    X_test = np.concatenate ( X_test_aux )\n",
    "    y_train = np.concatenate ( y_train_aux )\n",
    "    y_test = np.concatenate ( y_test_aux )\n",
    "    \n",
    "    return X_train , X_test , y_train , y_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "In this section we will run a Cross Validation routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from tpot import TPOTClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = preprocess()\n",
    "\n",
    "# tpot = TPOTClassifier(generations=5, population_size=20, \n",
    "#                       verbosity=2,max_eval_time_mins=20,\n",
    "#                       max_time_mins=100,scoring='f1_micro',\n",
    "#                       random_state = 17)\n",
    "# tpot.fit(X_train, y_train)\n",
    "# print(tpot.score(X_test, y_test))\n",
    "# tpot.export('FinalPipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import  RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import  XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train and test a classifier\n",
    "\n",
    "# Pass in the classifier so we can iterate over many seed later.\n",
    "def train_and_test(X_tr, y_tr, X_v, well_v, clf):\n",
    "    \n",
    "    # Feature normalization\n",
    "    scaler = preprocessing.RobustScaler(quantile_range=(25.0, 75.0)).fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_v = scaler.transform(X_v)\n",
    "    \n",
    "    clf.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Test classifier\n",
    "    y_v_hat = clf.predict(X_v)\n",
    "    \n",
    "    # Clean isolated facies for each well\n",
    "    for w in np.unique(well_v):\n",
    "        y_v_hat[well_v==w] = medfilt(y_v_hat[well_v==w], kernel_size=5)\n",
    "    \n",
    "    return y_v_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train and test a classifier\n",
    "\n",
    "# Pass in the classifier so we can iterate over many seed later.\n",
    "def train_and_test_non_validation(X_tr, y_tr, X_v, well_v, clf):\n",
    "    \n",
    "    # Feature normalization\n",
    "    scaler = preprocessing.RobustScaler(quantile_range=(25.0, 75.0)).fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_v = scaler.transform(X_v)\n",
    "    \n",
    "    clf.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Test classifier\n",
    "    y_v_hat = clf.predict(X_v)\n",
    "    \n",
    "    # Clean isolated facies for each well\n",
    "#     for w in np.unique(well_v):\n",
    "#         y_v_hat[well_v==w] = medfilt(y_v_hat[well_v==w], kernel_size=5)\n",
    "    \n",
    "    return y_v_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n"
     ]
    }
   ],
   "source": [
    "#Load testing data\n",
    "test_data = pd.read_csv('../validation_data_nofacies.csv')\n",
    "\n",
    "    # Train classifier\n",
    "    #clf = make_pipeline(make_union(VotingClassifier([(\"est\", ExtraTreesClassifier(criterion=\"gini\", max_features=1.0, n_estimators=500))]), FunctionTransformer(lambda X: X)), XGBClassifier(learning_rate=0.73, max_depth=10, min_child_weight=10, n_estimators=500, subsample=0.27))\n",
    "    #clf =  make_pipeline( KNeighborsClassifier(n_neighbors=5, weights=\"distance\") ) \n",
    "    #clf = make_pipeline(MaxAbsScaler(),make_union(VotingClassifier([(\"est\", RandomForestClassifier(n_estimators=500))]), FunctionTransformer(lambda X: X)),ExtraTreesClassifier(criterion=\"entropy\", max_features=0.0001, n_estimators=500))\n",
    "    # * clf = make_pipeline( make_union(VotingClassifier([(\"est\", BernoulliNB(alpha=60.0, binarize=0.26, fit_prior=True))]), FunctionTransformer(lambda X: X)),RandomForestClassifier(n_estimators=500))\n",
    "\n",
    "# # Prepare training data\n",
    "# X_tr = X\n",
    "# y_tr = y\n",
    "\n",
    "# # Augment features\n",
    "# X_tr, padded_rows = augment_features(X_tr, well, depth)\n",
    "\n",
    "# # Removed padded rows\n",
    "# X_tr = np.delete(X_tr, padded_rows, axis=0)\n",
    "# y_tr = np.delete(y_tr, padded_rows, axis=0) \n",
    "\n",
    "# Prepare test data\n",
    "well_ts = test_data['Well Name'].values\n",
    "depth_ts = test_data['Depth'].values\n",
    "X_ts = test_data[feature_names].values\n",
    "\n",
    "\n",
    "    \n",
    "y_pred = []\n",
    "print('.' * 100)\n",
    "for seed in range(3):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Hold out two wells\n",
    "    ind_shk = np.array(data[data['Well Name']=='SHANKLE'].index)\n",
    "    ind_chr = np.array(data[data['Well Name']=='CHURCHMAN BIBLE'].index)\n",
    "    ind_ho_set = np.append(ind_shk,ind_chr)\n",
    "\n",
    "    # Make training data.\n",
    "    X_train, padded_rows = augment_features(X, well, depth)\n",
    "    y_train = y\n",
    "    X_test_nv = np.take(X_train, ind_ho_set, axis=0) \n",
    "    y_test_nv = np.take(y_train, ind_ho_set, axis=0) \n",
    "    X_train_nv = np.delete(X_train, ind_ho_set, axis=0)\n",
    "    y_train_nv = np.delete(y_train, ind_ho_set, axis=0)\n",
    "    \n",
    "    X_train_nv = np.delete(X_train_nv, padded_rows, axis=0)\n",
    "    y_train_nv = np.delete(y_train_nv, padded_rows, axis=0) \n",
    "    \n",
    "    X_test_nv = np.delete(X_test_nv, padded_rows, axis=0)\n",
    "    y_test_nv = np.delete(y_test_nv, padded_rows, axis=0)\n",
    "\n",
    "    # Train classifier  \n",
    "    clf = make_pipeline(XGBClassifier(learning_rate=0.12,\n",
    "                                      max_depth=3,\n",
    "                                      min_child_weight=10,\n",
    "                                      n_estimators=150,\n",
    "                                      seed=seed,\n",
    "                                      colsample_bytree=0.9))\n",
    "\n",
    "    \n",
    "    \n",
    "    # Make blind data.\n",
    "#     X_test, _ = augment_features(X_ts, well_ts, depth_ts)\n",
    "    \n",
    "\n",
    "    # Train and test.\n",
    "#     y_ts_hat = train_and_test(X_train, y_train, X_test, well_ts, clf)\n",
    "    \n",
    "    # Collect result.\n",
    "#     y_pred.append(y_ts_hat)\n",
    "#     print('|', end='')\n",
    "    \n",
    "# np.save('LA_Team_100_realizations.npy', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{937, 3745, 3784}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(padded_rows) & set(ind_ho_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 28)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_nv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(853,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_ho_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4149,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_nv, X_test_nv, y_train_nv, y_test_nv = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n",
    "# np.delete(X_train, ind_ho_set, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = train_and_test_non_validation(X_train_nv, y_train_nv, X_test_nv, well_ts, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.571764705882\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "tot = 0\n",
    "for i, entry in enumerate(confusion_matrix(y_pred,y_test_nv)):\n",
    "    for j, e in enumerate(entry):\n",
    "        if i == j:\n",
    "            correct += e\n",
    "        tot += e\n",
    "print(correct/tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57176470588235295"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred,y_test_nv, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57176470588235295"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred,y_test_nv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attempt at using LSTM for including influence of previous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature windows concatenation function\n",
    "def augment_features_window(X, N_neig):\n",
    "    \n",
    "    # Parameters\n",
    "    N_row = X.shape[0]\n",
    "    N_feat = X.shape[1]\n",
    "\n",
    "    # Zero padding\n",
    "    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))\n",
    "\n",
    "    # Loop over windows\n",
    "    X_aug = np.zeros((N_row, (2*N_neig+1), N_feat))\n",
    "    for r in np.arange(N_row)+N_neig:\n",
    "        this_row = []\n",
    "        for c in np.arange(-N_neig,N_neig+1):\n",
    "            this_row = np.hstack((this_row, X[r+c]))\n",
    "#         print(this_row.shape)\n",
    "        this_row.shape = ((2*N_neig+1), this_row.size // (2*N_neig+1))\n",
    "#         print(this_row)\n",
    "        X_aug[r-N_neig] = this_row\n",
    "\n",
    "    return X_aug\n",
    "\n",
    "\n",
    "# Feature augmentation function\n",
    "def augment_features(X, well, depth, N_neig=12):\n",
    "    \n",
    "    # Augment features\n",
    "    X_aug = np.zeros((X.shape[0], (N_neig*2+1), X.shape[1]))\n",
    "    for w in np.unique(well):\n",
    "        w_idx = np.where(well == w)[0]\n",
    "        X_aug_win = augment_features_window(X[w_idx, :], N_neig)\n",
    "#         X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])\n",
    "        X_aug[w_idx, :] = X_aug_win\n",
    "    \n",
    "    # Find padded rows\n",
    "    padded_rows = np.unique(np.where(X_aug[:, 0:7] == np.zeros((1, 7)))[0])\n",
    "    \n",
    "    return X_aug, padded_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('LSTM_acc_74.h5', overwrite=True)\n",
    "# y_pred = model.predict(X_test_nv_LSTM, batch_size=20, verbose=0)\n",
    "# y_test_LSTM_ct.shape\n",
    "# predicted_classes = np.argmax(y_pred, axis=1)\n",
    "# class_labels = np.argmax(y_test_LSTM_ct, axis=1)\n",
    "# f1_score(predicted_classes,class_labels, average='micro') # micro is the same as accuracy in this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection to apply LSTM w held out well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "data = pd.read_csv('../facies_vectors.csv')\n",
    "# data = pd.read_csv('../ShiangYong/facies_vectors_imputedPE.csv')\n",
    "# Parameters\n",
    "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "facies_names = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS', 'WS', 'D', 'PS', 'BS']\n",
    "facies_colors = ['#F4D03F', '#F5B041','#DC7633','#6E2C00', '#1B4F72','#2E86C1', '#AED6F1', '#A569BD', '#196F3D']\n",
    "\n",
    "# data.dropna(inplace=True)\n",
    "# Store features and labels\n",
    "X = data[feature_names].values \n",
    "y = data['Facies'].values \n",
    "\n",
    "# Store well labels and depths\n",
    "well = data['Well Name'].values\n",
    "depth = data['Depth'].values\n",
    "# X = np.array(pd.DataFrame(X).dropna())\n",
    "# Fill 'PE' missing values with mean\n",
    "imp = preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "\n",
    "# NEEDS TO BE CHANGED, SCALING SHOULD NOT BE DETERMINED FROM TESTING AND TRAINING SET, ONLY TRAINING\n",
    "# scaler = preprocessing.RobustScaler(quantile_range=(25.0, 75.0)).fit(X)\n",
    "# X = scaler.transform(X)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "# X_test_nv_LSTM = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get indicies of wells that will make up hold out set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, padded_rows = augment_features(X, well, depth)\n",
    "y_train = y\n",
    "X_test_LSTM = np.take(X_train, ind_ho_set, axis=0) \n",
    "y_test_LSTM = np.take(y_train, ind_ho_set, axis=0) \n",
    "X_train_LSTM = np.delete(X_train, ind_ho_set, axis=0)\n",
    "y_train_LSTM = np.delete(y_train, ind_ho_set, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4149, 25, 7)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_LSTM = X_test_LSTM[0:840]\n",
    "y_test_LSTM = y_test_LSTM[0:840]\n",
    "X_train_LSTM = X_train_LSTM[0:3280]\n",
    "y_train_LSTM = y_train_LSTM[0:3280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_nv_LSTM, X_test_nv_LSTM, y_train_nv_LSTM, y_test_nv_LSTM = train_test_split(X_train[0:4000], y_train[0:4000], test_size=0.3, random_state=42)\n",
    "y_train_LSTM = y_train_LSTM - 1\n",
    "y_test_LSTM = y_test_LSTM - 1\n",
    "y_train_LSTM_ct = to_categorical(np.array(y_train_LSTM), num_classes=None)\n",
    "y_test_LSTM_ct = to_categorical(np.array(y_test_LSTM), num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3280 samples, validate on 840 samples\n",
      "Epoch 1/30\n",
      "3280/3280 [==============================] - 6s 2ms/step - loss: 1.6064 - acc: 0.3774 - val_loss: 1.5734 - val_acc: 0.4536\n",
      "Epoch 2/30\n",
      "3280/3280 [==============================] - 5s 2ms/step - loss: 1.3494 - acc: 0.4445 - val_loss: 1.5114 - val_acc: 0.4690\n",
      "Epoch 3/30\n",
      "3280/3280 [==============================] - 5s 2ms/step - loss: 1.2369 - acc: 0.4814 - val_loss: 1.4920 - val_acc: 0.4560\n",
      "Epoch 4/30\n",
      "3280/3280 [==============================] - 6s 2ms/step - loss: 1.1735 - acc: 0.5046 - val_loss: 1.4417 - val_acc: 0.4321\n",
      "Epoch 5/30\n",
      "3280/3280 [==============================] - 7s 2ms/step - loss: 1.1223 - acc: 0.5274 - val_loss: 1.4163 - val_acc: 0.4060\n",
      "Epoch 6/30\n",
      "3280/3280 [==============================] - 4s 1ms/step - loss: 1.0454 - acc: 0.5573 - val_loss: 1.4403 - val_acc: 0.4214\n",
      "Epoch 7/30\n",
      "3280/3280 [==============================] - 4s 1ms/step - loss: 1.0187 - acc: 0.5826 - val_loss: 1.4371 - val_acc: 0.4250\n",
      "Epoch 8/30\n",
      "3280/3280 [==============================] - 4s 1ms/step - loss: 0.9741 - acc: 0.5905 - val_loss: 1.4768 - val_acc: 0.4548\n",
      "Epoch 9/30\n",
      "3280/3280 [==============================] - 5s 2ms/step - loss: 0.9411 - acc: 0.6098 - val_loss: 1.4648 - val_acc: 0.4464\n",
      "Epoch 10/30\n",
      "3280/3280 [==============================] - 5s 2ms/step - loss: 0.9201 - acc: 0.6171 - val_loss: 1.4980 - val_acc: 0.4226\n",
      "Epoch 11/30\n",
      "3280/3280 [==============================] - 5s 2ms/step - loss: 0.8867 - acc: 0.6338 - val_loss: 1.5863 - val_acc: 0.4417\n",
      "Epoch 12/30\n",
      "3280/3280 [==============================] - 5s 2ms/step - loss: 0.8792 - acc: 0.6466 - val_loss: 1.4540 - val_acc: 0.4536\n",
      "Epoch 13/30\n",
      "3280/3280 [==============================] - 5s 2ms/step - loss: 0.8776 - acc: 0.6399 - val_loss: 1.5011 - val_acc: 0.4405\n",
      "Epoch 14/30\n",
      "3280/3280 [==============================] - 5s 2ms/step - loss: 0.8456 - acc: 0.6491 - val_loss: 1.4694 - val_acc: 0.4679\n",
      "Epoch 15/30\n",
      "3280/3280 [==============================] - 6s 2ms/step - loss: 0.8390 - acc: 0.6567 - val_loss: 1.5206 - val_acc: 0.4250\n",
      "Epoch 16/30\n",
      "3280/3280 [==============================] - 5s 2ms/step - loss: 0.7924 - acc: 0.6689 - val_loss: 1.4990 - val_acc: 0.4440\n",
      "Epoch 17/30\n",
      "3280/3280 [==============================] - 4s 1ms/step - loss: 0.7398 - acc: 0.6951 - val_loss: 1.5132 - val_acc: 0.4452\n",
      "Epoch 18/30\n",
      "3280/3280 [==============================] - 5s 2ms/step - loss: 0.7075 - acc: 0.7079 - val_loss: 1.5197 - val_acc: 0.4750\n",
      "Epoch 19/30\n",
      "3280/3280 [==============================] - 4s 1ms/step - loss: 0.6991 - acc: 0.7213 - val_loss: 1.7356 - val_acc: 0.4476\n",
      "Epoch 20/30\n",
      "3280/3280 [==============================] - 5s 1ms/step - loss: 0.7167 - acc: 0.7128 - val_loss: 1.6187 - val_acc: 0.4476\n",
      "Epoch 21/30\n",
      "3280/3280 [==============================] - 5s 2ms/step - loss: 0.6947 - acc: 0.7201 - val_loss: 1.7488 - val_acc: 0.4464\n",
      "Epoch 22/30\n",
      "3280/3280 [==============================] - 4s 1ms/step - loss: 0.6693 - acc: 0.7262 - val_loss: 1.6334 - val_acc: 0.4381\n",
      "Epoch 23/30\n",
      "3280/3280 [==============================] - 5s 1ms/step - loss: 0.6590 - acc: 0.7396 - val_loss: 1.7195 - val_acc: 0.4548\n",
      "Epoch 24/30\n",
      "3280/3280 [==============================] - 4s 1ms/step - loss: 0.6249 - acc: 0.7494 - val_loss: 1.7590 - val_acc: 0.4655\n",
      "Epoch 25/30\n",
      "3280/3280 [==============================] - 5s 2ms/step - loss: 0.6136 - acc: 0.7598 - val_loss: 1.6532 - val_acc: 0.4524\n",
      "Epoch 26/30\n",
      "3280/3280 [==============================] - 4s 1ms/step - loss: 0.5935 - acc: 0.7637 - val_loss: 1.6634 - val_acc: 0.4798\n",
      "Epoch 27/30\n",
      "3280/3280 [==============================] - 4s 1ms/step - loss: 0.5905 - acc: 0.7643 - val_loss: 1.7238 - val_acc: 0.4702\n",
      "Epoch 28/30\n",
      "3280/3280 [==============================] - 5s 1ms/step - loss: 0.5563 - acc: 0.7814 - val_loss: 1.6458 - val_acc: 0.4893\n",
      "Epoch 29/30\n",
      "3280/3280 [==============================] - 5s 2ms/step - loss: 0.5526 - acc: 0.7872 - val_loss: 1.8320 - val_acc: 0.4214\n",
      "Epoch 30/30\n",
      "3280/3280 [==============================] - 5s 1ms/step - loss: 0.5473 - acc: 0.7951 - val_loss: 1.6977 - val_acc: 0.4869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f25a92c1080>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "# Notes\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_dim = 7 # Features\n",
    "timesteps = 25 # 25 is best so far observed\n",
    "num_classes = 9\n",
    "batch_size = 20 # 20 is best so far observed\n",
    "\n",
    "# Expected input batch shape: (batch_size, timesteps, data_dim)\n",
    "# Note that we have to provide the full batch_input_shape since the network is stateful.\n",
    "# the sample of index i in batch k is the follow-up for the sample i in batch k-1.\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(LSTM(40, stateful=True, return_sequences=True,\n",
    "               batch_input_shape=(batch_size, timesteps, data_dim)))\n",
    "\n",
    "# model.add(LSTM(40, return_sequences=True, stateful=True))\n",
    "\n",
    "model.add(LSTM(40, stateful=True))\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Nadam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_LSTM, y_train_LSTM_ct,\n",
    "          batch_size=batch_size, epochs=30, shuffle=False,\n",
    "          validation_data=(X_test_LSTM, y_test_LSTM_ct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
